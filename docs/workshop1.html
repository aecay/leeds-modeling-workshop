<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Introduction to statistical modeling</title>
<meta name="author" content="(Aaron Ecay)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://aecay.github.io/leeds-modeling-workshop/revealjs/css/reveal.css"/>

<link rel="stylesheet" href="https://aecay.github.io/leeds-modeling-workshop/revealjs/css/theme/black.css" id="theme"/>


<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://aecay.github.io/leeds-modeling-workshop/revealjs/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1>Introduction to statistical modeling</h1>
</section>

<section>
<section id="slide-org68fcb72">
<h2 id="org68fcb72">Introduction</h2>
<div class="outline-text-2" id="text-org68fcb72">
</div>
</section>
<section id="slide-orgc8c3c17">
<h3 id="orgc8c3c17">Goals of this talk</h3>
<ul>
<li class="fragment">Preparation for mixed-effects workshop in June</li>
<li class="fragment">Provide an overview of topics in mixed-effects modeling</li>
<li class="fragment">“Zero to sixty”</li>
<li class="fragment">(unfortunately) not an in-depth tutorial
<ul>
<li class="fragment">Though we will be interacting with R</li>

</ul></li>
<li class="fragment">Aim: provide participants with the “lay of the land” in order to ask the right questions</li>

</ul>

</section>
<section id="slide-orgc49eebd">
<h3 id="orgc49eebd">Background on modeling</h3>
<div class="outline-text-3" id="text-orgc49eebd">
</div>
</section>
<section id="slide-org5213691">
<h4 id="org5213691">What modeling is</h4>
<ul>
<li class="fragment">There are a two fundamental ways of looking at statistical modeling</li>
<li class="fragment"><b>Engineering solution</b> give me the correct answer, it doesnʼt matter how you get there</li>
<li class="fragment"><b>Scientific solution</b> tell me how the world works</li>
<li class="fragment">The tension between these ideas underlies a lot of the groundwork of using statistical models for research</li>

</ul>

</section>
<section id="slide-org6eef94c">
<h4 id="org6eef94c">Another way to look at modeling</h4>
<ul>
<li class="fragment">Thereʼs another way to look at statistical models, however</li>
<li class="fragment">Dodges the tension between the scientific and engineering approaches</li>
<li class="fragment">Illuminates some aspects of the structure of modeling problems</li>
<li class="fragment">Models as <b>data compression</b></li>

</ul>

</section>
<section id="slide-org3ebe084">
<h4 id="org3ebe084">Data compression: the basics</h4>
<ul>
<li class="fragment">One kind of familiar data compression: ZIP files
<ul>
<li class="fragment">Take a computer file, make it smaller</li>

</ul></li>
<li class="fragment">Letʼs try to develop our own miniature compression scheme</li>

</ul>

<blockquote style="font-size:40%">
<p>
Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, ‘and what is the use of a book,’ thought Alice ‘without pictures or conversations?’
</p>
</blockquote>

</section>
<section >

<p>
the &rarr; z
</p>

<blockquote>
<p>
Alice was beginning to get very tired of sitting by her sister on <b>z</b> bank, and of having nothing to do: once or twice she had peeped into <b>z</b> book her sister was reading, but it had no pictures or conversations in it, ‘and what is <b>z</b> use of a book,’ thought Alice ‘without pictures or conversations?’
</p>
</blockquote>

</section>
<section >

<p>
ha &rarr; x
</p>

<blockquote>
<p>
Alice was beginning to get very tired of sitting by her sister on <b>z</b> bank, and of <b>x</b>ving nothing to do: once or twice she <b>x</b>d peeped into <b>z</b> book her sister was reading, but it <b>x</b>d no pictures or conversations in it, ‘and w<b>x</b>t is <b>z</b> use of a book,’ thought Alice ‘without pictures or conversations?’
</p>
</blockquote>

</section>
<section >

<p>
e␣ &rarr; y and vice versa
</p>

<blockquote>
<p>
Alic<b>y</b>was beginning to get ver<b>e␣</b> tired of sitting b<b>e␣</b> her sister on <b>z</b> bank, and of <b>x</b>ving nothing to do: onc<b>y</b>or twic<b>y</b>sh<b>y</b><b>x</b>d peeped into <b>z</b> book her sister was reading, but it <b>x</b>d no pictures or conversations in it, ‘and w<b>x</b>t is <b>z</b> us<b>y</b>of a book,’ thought Alic<b>y</b>‘without pictures or conversations?’
</p>
</blockquote>

</section>
<section id="slide-org25783c9">
<h4 id="org25783c9">Notes</h4>
<ul>
<li class="fragment">“X” is <b>more likely</b> to appear than “Y” – we are using the language of probability to describe our model</li>
<li class="fragment">The model is based on assumptions about the world (the English language) that could turn out to be wrong</li>
<li class="fragment">This is a staatistical model!</li>

</ul>

</section>
<section id="slide-orgd379b9f">
<h4 id="orgd379b9f">Lossless vs. lossy compression</h4>
<ul>
<li class="fragment">This toy example (as well as the ZIP file format) are examples of lossless compression – you can recover the original data with perfect fidelity</li>
<li class="fragment">Not all compression is lossless; some is lossy</li>

</ul>

</section>
<section id="slide-org235f6fb">
<h4 id="org235f6fb">Lossy compression example</h4>
<ul>
<li class="fragment">A familiar example of a lossy compression format is JPEG images</li>
<li class="fragment">Not all the information your eye sees is in the image file</li>
<li class="fragment">JPEG has a model of how images look</li>
<li class="fragment">When the world doesnʼt line up to this model, the result looks bad</li>

</ul>

</section>
<section >


<div class="figure">
<p><img src="hot-air-balloon.jpg" alt="hot-air-balloon.jpg" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-orgca852a9">
<h2 id="orgca852a9">Compression and models</h2>
<ul>
<li class="fragment">The kinds of statistical models weʼll be talking about in these seminars are lossy</li>
<li class="fragment">They divide the information in the data into two parts
<ul>
<li class="fragment">Parameters</li>
<li class="fragment">Residuals</li>

</ul></li>
<li class="fragment">R<sup>2</sup> is a measure of how much of the variance in the data is explained by the parameters</li>

</ul>

</section>
<section id="slide-org54486c7">
<h3 id="org54486c7">Exercise 1</h3>
<ul>
<li class="fragment">Open up R on your machine</li>
<li class="fragment">Run this code:</li>

</ul>

<div class="org-src-container">

<pre  class="src src-R">install.pacakges<span style="color: #b0c4de;">(</span><span style="color: #ffa07a;">"shiny"</span><span style="color: #b0c4de;">)</span>
<span style="color: #7fffd4;">library</span><span style="color: #b0c4de;">(</span>shiny<span style="color: #b0c4de;">)</span>
runGitHub<span style="color: #b0c4de;">(</span><span style="color: #ffa07a;">"aecay/leeds-modeling-workshop"</span>, subdir <span style="color: #7fffd4;">=</span> <span style="color: #ffa07a;">"w1/"</span><span style="color: #b0c4de;">)</span>
</pre>
</div>

</section>
<section id="slide-orge7db8a3">
<h3 id="orge7db8a3">What are we doing?</h3>
<ul>
<li class="fragment">Dataset of reaction times in a lexical decision task, from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3278621/">the British Lexicon Project</a>.</li>
<li class="fragment">Random subset of 500 words (to keep computations managable)</li>
<li class="fragment">Predictors:</li>

</ul>



<dl style="font-size:30%">
<dt class="fragment">lexicality</dt><dd class="fragment">word or non-word</dd>
<dt class="fragment">nletters</dt><dd class="fragment">number of letters in the stimulus</dd>
<dt class="fragment">subtlex.frequency</dt><dd class="fragment">frequency of the stimulus in the SUBTLEX corpus</dd>
<dt class="fragment">summed.bigram</dt><dd class="fragment">sum of bigram frequencies in the word</dd>
<dt class="fragment">OLD20</dt><dd class="fragment">a measure of neighborhood density</dd>
<dt class="fragment">part3</dt><dd class="fragment">remainder when dividing the participantʼs study ID by 3</dd>
<dt class="fragment">lett.odd</dt><dd class="fragment">did the word start with an odd-numbered letter of the alphabet</dd>

</dl>


<ul>
<li class="fragment">See what you can come up with&#x2026;</li>

</ul>

</section>
<section id="slide-org37fd4f7">
<h3 id="org37fd4f7">What have we learned</h3>
<ul>
<li class="fragment">Adding linguistically useful predictors to the model increases the R<sup>2</sup></li>
<li class="fragment">But so does adding completely random predictors!</li>
<li class="fragment">In fact, adding an extra predictor will <b>always</b> increase R<sup>2</sup> (sometimes just imperceptibly)</li>
<li class="fragment">So, we need more sophisticated means than R<sup>2</sup> to determine what predictors to keep in a model
<ul>
<li class="fragment">Topic of session 3</li>

</ul></li>
<li class="fragment">Now, letʼs shift gears and think about what a predictor is</li>

</ul>

</section>
</section>
<section>
<section id="slide-org949db79">
<h2 id="org949db79">Translating hypotheses to models</h2>
<ul style="font-size:80%">
<li class="fragment">In the previous exercise, we had a very basic notion of predictors</li>
<li class="fragment">Now we want to make this more explicit</li>
<li class="fragment">The basic regression model: linear regression
<ul>
<li class="fragment">\(\hat{y}_i = \beta x_i + \epsilon_i\)</li>

</ul></li>
<li class="fragment">Each i is one observation; x is a vector of features; &beta; is fit by the model</li>
<li class="fragment">We pick what the elements of \(x_i\) are
<ul>
<li class="fragment">The structure of x corresponds to the structure of our hypotheses</li>

</ul></li>

</ul>

</section>
<section id="slide-org7203851">
<h3 id="org7203851">(In)dependent variables</h3>
<ul>
<li class="fragment">The most common way of describing the structure of a statistical model uses the terms “dependent variable” and “independent variable”
<dl>
<dt class="fragment">dependent variable</dt><dd class="fragment">the y in the equation.  The value of y depends on the value of x.</dd>
<dt class="fragment">independent variable</dt><dd class="fragment">the x in the equation.  It does not depend on y.</dd>

</dl></li>

</ul>


</section>
<section >

<ul>
<li class="fragment">This terminology is a mess
<ul>
<li class="fragment">It implies that x is causally prior to y, but this is not necessarily the case</li>
<li class="fragment">It implies that the xʼs are independent of each other, but (ditto)</li>
<li class="fragment">Itʼs confusing</li>

</ul></li>

</ul>

</section>
<section >

<ul>
<li class="fragment">Weʼre stuck with it, though</li>
<li class="fragment">Sometimes the independent variables (xʼs) are also referred to as predictors, and y as the outcome.
A bit clearer, but not totally standard.
<ul>
<li class="fragment">Iʼll try to use this terminology for the presentation</li>

</ul></li>

</ul>

</section>
<section id="slide-org6e60097">
<h3 id="org6e60097">Linear terms</h3>
<ul>
<li class="fragment">The simplest statistical model is of a linear relationship between a predictor and an outcome
<ul>
<li class="fragment">F° = \(\frac{9}{5}\) C° + 32</li>

</ul></li>
<li class="fragment">Because of this, linear models are the most often used in science
<ul>
<li class="fragment">Not necessarily because linear relationships are underlyingly true</li>

</ul></li>

</ul>

</section>
<section >


<div class="figure">
<p><img src="extrapolating.png" alt="extrapolating.png" />
</p>
</div>

</section>
<section id="slide-orgcf5ea82">
<h4 id="orgcf5ea82">Imperfect but useful</h4>
<ul style="font-size:70%">
<li class="fragment">Nonetheless, linear regression can be useful to describe trends in the data</li>
<li class="fragment">R has a special object for describing the structure of models: the formula
<ul>
<li class="fragment"><code>outcome ~ predictor1 + predictor2 + ...</code></li>

</ul></li>
<li class="fragment">This resembles, but is not exactly, the mathematical formula for the regression
<ul>
<li class="fragment">Itʼs missing the intercept term: the value that <code>outcome</code> will take on when all the predictors are 0</li>
<li class="fragment">Itʼs missing &epsilon;</li>

</ul></li>
<li class="fragment">In order to fit a Linear Model in R, use the <code>lm</code> function
<ul>
<li class="fragment"></li>

</ul></li>

</ul>

</section>
<section id="slide-orgd172c69">
<h4 id="orgd172c69">Fitting linear models in R</h4>
<ul>
<li class="fragment">The output of <code>lm</code> isnʼt maximally informative</li>

</ul>

<div class="org-src-container">

<pre  class="src src-R">lm<span style="color: #b0c4de;">(</span>rt ~ nletters, data <span style="color: #7fffd4;">=</span> dat<span style="color: #b0c4de;">)</span>
</pre>
</div>

<pre class="example">
Call:
lm(formula = rt ~ nletters, data = dat)

Coefficients:
(Intercept)     nletters
     567.10        10.48
</pre>

</section>
<section id="slide-orgf08e496">
<h4 id="orgf08e496">A better way</h4>
<ul>
<li class="fragment">So we ask for the summary of the linear model
<ul>
<li class="fragment">(Counterintuitively, the summary is longer and more informative than the model itself)</li>
<li class="fragment">Lots of objects in R can be summarized, not only models</li>

</ul></li>

</ul>

<div class="org-src-container">

<pre  class="src src-R">summary<span style="color: #b0c4de;">(</span>lm<span style="color: #b0c4de;">(</span>rt ~ nletters, data <span style="color: #7fffd4;">=</span> dat<span style="color: #b0c4de;">))</span>
</pre>
</div>

<pre class="example" style="font-size:30%">
Call:
lm(formula = rt ~ nletters, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max
-414.94 -132.50  -52.48   73.06 1706.50

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 567.1043     6.5274   86.88   &lt;2e-16 ***
nletters     10.4799     0.9756   10.74   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 202.5 on 16952 degrees of freedom
  (3412 observations deleted due to missingness)
Multiple R-squared:  0.00676,	Adjusted R-squared:  0.006702
F-statistic: 115.4 on 1 and 16952 DF,  p-value: &lt; 2.2e-16
</pre>

</section>
<section id="slide-org71ca2ba">
<h4 id="org71ca2ba">Trying it ourselves</h4>
<ul>
<li class="fragment">Try to fit a model that has both <code>nletters</code> and <code>summed.bigram</code> as predictors</li>
<li class="fragment">What do you notice?</li>

</ul>

</section>
<section id="slide-org59f6573">
<h4 id="org59f6573">P-values</h4>
<ul>
<li class="fragment">There are two types of p-values in the model output</li>
<li class="fragment">The first: associated with each predictor
<ul>
<li class="fragment">A statistical test: does the model fit better with this predictor or without it?</li>

</ul></li>
<li class="fragment">The second: associated with the model
<ul>
<li class="fragment">Does this model fit better than no model at all</li>
<li class="fragment">Unless you are doing something really silly, this will always be very small</li>

</ul></li>

</ul>

</section>
<section >

<ul>
<li class="fragment">Experiment with adding and subtracting predictors in Exercise 2
<ul>
<li class="fragment">What do you notice about the p-values?
Is it possible to give one single “true” p-value for each predictor?</li>

</ul></li>

</ul>

</section>
<section id="slide-orgaedcc81">
<h3 id="orgaedcc81">Nonlinear terms</h3>
<ul>
<li class="fragment">The popularity of linear regression raises the question: what about cases where the linearity assumption doesnʼt hold?</li>
<li class="fragment">Weʼll consider two cases:
<ul>
<li class="fragment">Non-numeric predictors</li>
<li class="fragment">Curvilinear relationships</li>

</ul></li>

</ul>

</section>
<section id="slide-org38907c8">
<h4 id="org38907c8">Nonnumeric predictors</h4>
<ul>
<li class="fragment">What if we are trying to predict reaction time by lexicality?</li>
<li class="fragment">575 + 10 * (is not a word) = ???</li>
<li class="fragment">What happens if we try this in the interactive model?</li>
<li class="fragment">One value is the default, the other is assigned a predictor</li>
<li class="fragment">What happens with a ternary value like <code>part3</code>?</li>
<li class="fragment">Is this the only way to do it?</li>

</ul>

</section>
<section id="slide-orgdf6c081">
<h4 id="orgdf6c081">Curvilinear predictors</h4>
<ul>
<li class="fragment"><p>
Itʼs also possible for a predictor to have a curvilinear relationship with an outcome
</p>


<div id="org0fcbabf" class="figure">
<p><object type="image/svg+xml" data="sickle-cell.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div></li>

<li class="fragment">This doesnʼt come up in our example dataset, but it is worth keeping in mind</li>

</ul>

</section>
<section id="slide-org904cef7">
<h3 id="org904cef7">Interaction terms</h3>
<ul>
<li class="fragment">A single predictor might have different effects in different contexts</li>
<li class="fragment">An example from our dataset: lexicality and bigram frequency</li>

</ul>


<div id="org5e39c57" class="figure">
<p><object type="image/svg+xml" data="interaction1.svg" class="fragment">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
<section >


<div class="figure">
<p><object type="image/svg+xml" data="interaction2.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
<section id="slide-orga4e070f">
<h4 id="orga4e070f">Whats going on here?</h4>
<ul>
<li class="fragment">(NB this is not an attempt to actually explain this phenomenon)</li>
<li class="fragment">Maybe: there is a difference in what speakers do for words and non-words</li>
<li class="fragment">For words:
<ul style="font-size:50%">
<li class="fragment">Look the word up by meaning</li>
<li class="fragment">“Hash table” algorithm</li>
<li class="fragment">Takes ~constant time</li>

</ul></li>
<li class="fragment">For non-words:
<ul style="font-size:50%">
<li class="fragment">Search through all the words you know to make sure itʼs not there</li>
<li class="fragment">“List search” algorithm</li>
<li class="fragment">Takes time proportional to the wordʼs length</li>

</ul></li>

</ul>

</section>
<section id="slide-org64dca63">
<h4 id="org64dca63">Illustration</h4>
<ul>
<li class="fragment">The most bigram-frequent non-word in the data is “trainstessed”
<ul style="font-size:50%">
<li class="fragment">It looks very word-like</li>
<li class="fragment">It contains meaningful morphemes (train, -ed)</li>
<li class="fragment">Itʼs long (12 letters, 3 syllables)</li>
<li class="fragment">It takes a (relatively) long time to satisfy ourselves that this is not in fact a word</li>

</ul></li>
<li class="fragment">Compare “gix”, one of the most bigram-infrequent words in the sample
<ul>
<li class="fragment">We can rapidly be sure itʼs not a word</li>

</ul></li>
<li class="fragment">(Aside: bigram frequency should probably be normalized by length)</li>

</ul>

</section>
<section id="slide-orgd578612">
<h4 id="orgd578612">Modeling</h4>
<ul>
<li class="fragment">Whatever the underlying reasons, we want our model to take this property of the data into account</li>
<li class="fragment"><p>
If we ignore it, we will just fit one effect for summed bigram frequency
</p>


<div class="figure">
<p><object type="image/svg+xml" data="interaction3.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div></li>

</ul>

</section>
<section id="slide-org03eb771">
<h4 id="org03eb771">Interactions in R</h4>
<ul>
<li class="fragment">In order to fit an interaction term in R, we use the multiplication notation: <code>predictor1*predictor2</code></li>
<li class="fragment">This is shorthand for three predictors:
<ul>
<li class="fragment"><code>predictor1</code></li>
<li class="fragment"><code>predictor2</code></li>
<li class="fragment">the two predictors multiplied together (notated <code>predictor1:predictor2</code>)</li>

</ul></li>

</ul>

</section>
<section >

<ul>
<li class="fragment">Look at Exercise 3, which is the same as Exercise 2 with the choice added for an interaction term
<ul>
<li class="fragment">Can you demonstrate that the <code>*</code> notation adds the predictors I said it should?  That is, that you can simply write <code>summed.bigram*lexicality</code>, not <code>summed.bigram*lexicality + summed.bigram + lexicality</code>?</li>
<li class="fragment">What happens to the p-values when you add the interaction term?</li>

</ul></li>

</ul>

</section>
<section id="slide-orgd8fc1f4">
<h3 id="orgd8fc1f4">Nonlinear outputs</h3>
<div class="outline-text-3" id="text-orgd8fc1f4">
</div>
</section>
<section id="slide-org39993b7">
<h4 id="org39993b7">Non-numeric outputs</h4>
<ul>
<li class="fragment">What if your output isnʼt numeric?</li>
<li class="fragment">Forced choice judgment task, corpus data (ing vs. inʼ)</li>
<li class="fragment">One possible answer: predict the “chance” of one outcome or the other
<ul>
<li class="fragment">If prediction is &gt; 0.5 guess “yes”, else “no”</li>

</ul></li>
<li class="fragment">This basically works</li>

</ul>

</section>
<section id="slide-org27ad7d4">
<h4 id="org27ad7d4">The logistic curve</h4>
<ul>
<li class="fragment">But not with a linear model</li>

</ul>


<div id="org9218f3f" class="figure">
<p><object type="image/svg+xml" data="logistic.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
<section id="slide-org3531427">
<h3 id="org3531427">Non-linear outputs</h3>
<ul>
<li class="fragment">What if the relationship between the input and the output is not linear?</li>
<li class="fragment">If I give my tomatoes 1 gallon of water, they grow a foot
<ul>
<li class="fragment">100 gallons &rarr; 100 feet?</li>

</ul></li>
<li class="fragment">The solution is transforming the data
<ul>
<li class="fragment">Square, square root, logarithm&#x2026;</li>
<li class="fragment">Sometimes you know what to use, sometimes you try to figure it out from the data</li>

</ul></li>
<li class="fragment">More on this topic coming up right now&#x2026;</li>

</ul>

</section>
</section>
<section>
<section id="slide-org00b2c3e">
<h2 id="org00b2c3e">Checking assumptions</h2>
<div class="outline-text-2" id="text-org00b2c3e">
</div>
</section>
<section id="slide-orgb311ea8">
<h3 id="orgb311ea8">Assumptions of linear models</h3>
<ul>
<li class="fragment">Letʼs get back to our data compression example for a moment</li>
<li class="fragment">Which is more compressed?</li>

</ul>

<div class="org-center">
<pre class="example">
6 6 6 6 5 4 2 6 6 3 1 4 5 1 2 2 1 2 3 6 4 6 4 6 2 1
2 6 2 2 6 1 6 1 6 3 6 6 2 2 2 4 3 5 5 3 5 2 3 4 4 6
2 4 4 4 6 4 2 1 5 4 4 3 2 5 5 3 1 2 1 4 1 3 6 4 5 3
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">5</td>
<td class="org-right">6</td>
</tr>

<tr>
<td class="org-right">16.6%</td>
<td class="org-right">16.6%</td>
<td class="org-right">16.6%</td>
<td class="org-right">16.6%</td>
<td class="org-right">16.6%</td>
<td class="org-right">16.6%</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-org0fe2dc7">
<h3 id="org0fe2dc7">Compression gone awry</h3>

<div class="figure">
<p><img src="snakesladders.jpg" alt="snakesladders.jpg" width="30%" />
</p>
</div>

<ul>
<li class="fragment">2 5 vs. 5 2</li>

</ul>

</section>
<section id="slide-orgad43ee4">
<h3 id="orgad43ee4">Residuals and compression</h3>
<ul>
<li class="fragment">Residuals in a model are a long list of random numbers
<ul>
<li class="fragment">They look like rolls of a die</li>

</ul></li>
<li class="fragment">They compress much better if order doesnʼt matter</li>
<li class="fragment">Important assumption of linear models: <i>homoskedastic residuals</i>
<ul>
<li class="fragment">“same variance”</li>

</ul></li>

</ul>

</section>
<section id="slide-orgd53a093">
<h3 id="orgd53a093">Plotting to check homoskedasticity</h3>
<ul>
<li class="fragment">Homoskedasticity can be checked on a fitted-residual plot</li>

</ul>


<div id="org1d00454" class="figure">
<p><object type="image/svg+xml" data="fitted-resid.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
<section >


<div class="figure">
<p><object type="image/svg+xml" data="fitted-resid-ln.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>

</section>
</section>
<section>
<section id="slide-orgfa79287">
<h2 id="orgfa79287">Homoskedasticity and mixed effects models</h2>
<div class="outline-text-2" id="text-orgfa79287">
</div>
</section>
<section id="slide-org8dc2609">
<h3 id="org8dc2609">The problem</h3>
<ul>
<li class="fragment">We can never, ever assume that residuals are homoskedastic in linguistics
<ul>
<li class="fragment">Speakers</li>
<li class="fragment">Words</li>

</ul></li>
<li class="fragment">We need to tell the model about this, or it will be misled</li>
<li class="fragment">The answer: mixed-effects models</li>

</ul>

</section>
<section id="slide-orgd3e1edc">
<h3 id="orgd3e1edc">Towards a solution</h3>
<ul>
<li class="fragment">Predictors in a model “control” for sources of variability</li>
<li class="fragment">One idea: letʼs just add another predictor to the model for subject, to “control” subject level-variability
<ul>
<li class="fragment"><code>outcome ~ predictor1 + predictor2 + subject</code></li>

</ul></li>

</ul>

</section>
<section >

<ul>
<li class="fragment">Whatʼs wrong with this idea?
<ul>
<li class="fragment">It introduces lots of parameters to the model</li>
<li class="fragment">It allows subjects to vary too much</li>

</ul></li>
<li class="fragment">This second one Iʼll call “parameter identification”
<ul>
<li class="fragment">Is an effect due to (e.g.) gender, or is it because Peter, Paul, and Mary are just different people?</li>

</ul></li>

</ul>

</section>
<section id="slide-org0141a59">
<h3 id="org0141a59">Mixed-effects models</h3>
<ul>
<li class="fragment">Intuition: most subjects are like the average subject</li>
<li class="fragment">This sounds trivial, but itʼs not
<ul>
<li class="fragment">Compare: most treatments are like the average treatment</li>
<li class="fragment">&#x2026;placebo, caffeine, alcohol, Adderall, chloroform</li>

</ul></li>
<li class="fragment">Add a predictor to the model per subject, but constrain it to follow a normal distribution</li>

</ul>

</section>
<section id="slide-org5e40c42">
<h3 id="org5e40c42">Benefits of mixed effects models</h3>
<ul>
<li class="fragment">Solves the many-parameters problem: we only need 2 (mean, variance) regardless of how many subjects we have</li>
<li class="fragment">Solves the parameter identification problem: only a certain amount of variance can be attributed to subject effect; the rest should be allocated to other predictors (or &epsilon;)</li>

</ul>

</section>
<section id="slide-orgc88d978">
<h3 id="orgc88d978">Mixed effects models: the terminological swamp</h3>
<ul>
<li class="fragment">Unfortunately, statisticians/practitioners donʼt have good, consistent vocab for talking about these models</li>
<li class="fragment">Fixed/random effects</li>
<li class="fragment">Mixed-effects</li>
<li class="fragment">Hierarchical models</li>
<li class="fragment">Because of this, one of the easiest ways to communicate about these models is through code</li>

</ul>

</section>
<section id="slide-org2f259e8">
<h3 id="org2f259e8">Mixed effect models in R</h3>
<ul>
<li class="fragment">The usual formula syntax is extended to represent mixed-effects models
<ul>
<li class="fragment">Technical note: Iʼm using the widespread <code>lme4</code> syntax in this talk</li>

</ul></li>

</ul>

<div class="org-center">
<p>
<code>outcome ~ predictors + (1 | subject)</code>
</p>
</div>

</section>
<section id="slide-org0d7bd0b">
<h3 id="org0d7bd0b">Experimenting with mixed effects models</h3>
<ul>
<li class="fragment">Switch over to exercise 5</li>
<li class="fragment">What you see: a comparison of the coefficients in a mixed and non-mixed model, plus the summary of the mixed model</li>
<li class="fragment">Focus on the latter: whatʼs different?
<ul>
<li class="fragment">Scaling of predictors</li>
<li class="fragment">Random effect estimates</li>
<li class="fragment">Where are the p-values?</li>

</ul></li>
<li class="fragment">Now look at the coefficient comparisons: how does mixed-effects-ness affect the results?</li>

</ul>

</section>
<section id="slide-orga1fef44">
<h3 id="orga1fef44">Results of mixed-effect models</h3>
<ul>
<li class="fragment">It looks like mixed effects models havenʼt been very revolutionary here</li>
<li class="fragment">We might have expected this
<ul>
<li class="fragment">Balanced design</li>
<li class="fragment">Many participants</li>
<li class="fragment">Many stimuli</li>
<li class="fragment">Itʼs not a bad thing!</li>

</ul></li>

</ul>

</section>
<section >

<ul>
<li class="fragment">Where are mixed-effects models more useful?
<ul>
<li class="fragment">Unbalanced designs</li>
<li class="fragment">Corpus work</li>
<li class="fragment">Clamping down on potentially spurious effects</li>
<li class="fragment">Conceptually better model</li>

</ul></li>

</ul>

</section>
<section id="slide-orgcc47a91">
<h3 id="orgcc47a91">Wrapping up</h3>
<ul>
<li class="fragment">Today
<ul>
<li class="fragment">Introduction to modeling issues</li>
<li class="fragment">Motivation of mixed-effects modeling</li>

</ul></li>
<li class="fragment">Next time
<ul>
<li class="fragment">How to get p-values back (or more accurately, how to compare models and evaluate the contribution of predictors)</li>
<li class="fragment">More details on constructing mixed-effects models to fit research scenarios</li>

</ul></li>
<li class="fragment">Thanks for listening!</li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://aecay.github.io/leeds-modeling-workshop/revealjs/lib/js/head.min.js"></script>
<script src="https://aecay.github.io/leeds-modeling-workshop/revealjs/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://aecay.github.io/leeds-modeling-workshop/revealjs/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://aecay.github.io/leeds-modeling-workshop/revealjs/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://aecay.github.io/leeds-modeling-workshop/revealjs/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://aecay.github.io/leeds-modeling-workshop/revealjs/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://aecay.github.io/leeds-modeling-workshop/revealjs/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
