#+title: Model comparison and selection
#+author: Aaron Ecay

#+property: header-args:R :session *stats-wkshp* :eval never-export

#+reveal_theme: black
#+reveal_trans: default

#+options: title:nil reveal_history:t num:nil toc:nil

#+macro: red @@html:<span style='color:red'>@@$1@@html:</span>@@
#+macro: blue @@html:<span style='color:blue'>@@$1@@html:</span>@@
#+macro: green @@html:<span style='color:green'>@@$1@@html:</span>@@

* prelim                                                           :noexport:
#+begin_src emacs-lisp
  (setq org-reveal-root "https://aecay.github.io/leeds-modeling-workshop/revealjs/"
        ;org-reveal-hlevel 2
        )

  (defun awe-ox-reveal-fragment (element val)
    (org-element-put-property element :attr_reveal `(":frag" ,val)))

  (defun awe-ox-reveal-fragment-lists (tree _backend _info)
    (org-element-map tree 'plain-list
      (lambda (l)
        (let* ((len (length (org-element-contents l)))
               (s (concat "(" (apply #'concat (cl-loop for i from 1 to len collect "t ")) ")")))
          (awe-ox-reveal-fragment l s))))
    (org-reveal-filter-parse-tree tree _backend _info))

  ;;; TODO: executes in the wrong order
  (setq org-export-filter-parse-tree-functions
        '(awe-ox-reveal-fragment-lists))

  (setq-local org-reveal-title-slide
              "<h1>%t</h1>")
#+end_src

* Introduction

- Last week: the motivation for mixed effects models
- Today: how to compare models to each other

* Model comparison methods

** Motivation

- We want to be able to compare models to each other
- Is model A better than model B
  - If model A = model B + one predictor, make inferences about that additional predictor
- As we saw last time, R^2 and p-values are not really good ways of doing this

** Cross-validation

- Perhaps the most intuitively understandable procedure for testing models
- Procedure:
  - Split the data in two
  - Fit the model on one portion of the data (training)
  - Evaluate how well it fits the other portion (testing)
- Lots of room for variation in how CV is performed

*** Types of crossvalidation

- Leave-one-out crossvalidation
  - Training = N-1 datapoints; testing = 1
  - Repeat N times for each datapoint
- k-fold CV
  - A typical value for k is 10
  - Training = N(k-1)/k; testing = N/k
  - Repeat k times
- Holdout CV
  - A typical value for p is 0.8
  - Training = pN; testing = (1-p)N
  - Not repeated

*** Disadvantages to cross-validation

- /Overfitting/ can be a concern, especially when N (dataset size) is small and P (number of predictors) is large
  - k-fold and LOO CV do not protect against overfitting
- Thereʼs not a good general rule of thumb for interpreting the difference in predictive performance between two models
- You probably have to program it yourself

#+reveal: split

- Conclusion: crossvalidation is almost never the right choice
  - That being said, it can be a last resort
  - Itʼs related to a couple of other things that weʼll discuss

** Likelihood ratio test

- Formally: a test of the hypothesis that certain parameter(s) of a model are fixed at certain values, rather than allowed to vary
- In practice: a test of whether parameters are equal to zero
- The LRT can only be validly applied to nested models: the predictors of one model are a proper subset of those in the other

** Information criteria

- Information criteria encompass a family of approaches which measure the goodness of fit of a model
- The mathematical structure of the criteria in this family is very simple
  - A measure of goodness of fit, the /likelihood/ of the data under the model
  - A measure of the modelʼs complexity
- Higher goodness of fit is better; as is lower complexity

*** Using ICs

- Information criteria are inherently relative; you can only use them to compare two models, not to decide how “good” a single model is in isolation
- Unlike LRT, ICs can be applied to non-nested models
- But they must be applied to models fit on the same data
  - Watch out for NAs in R

*** Interpreting ICs

- The model with a lower IC fits better
- The difference between the two modelsʼ IC indicates how much better the fit is

#+reveal: split

- Unlike p-value based methods, there is no cutoff for IC differences, but rather a rule of thumb
  | Difference | Interpretation                        |
  | 0–2        | No/only weak evidence of a difference |
  | 2–6        | Some evidence of a difference         |
  | 6–10       | Strong evidence                       |
  | 10+        | Very strong evidence                  |

*** AIC

- Akaike Information Criterion
- First of the family to be formulated
- \Delta{}AIC = asymptotic measure of how much more information about the data-generating process one model captures over another
- Does not assume that the true model is in the set of candidates
- More open to additional predictors than the BIC
- Requires small-sample correction (=AICcmodavg= R package)

*** BIC

- Bayesian information criterion
- \Delta{}BIC = asymptotically related to the Bayes factor between two models
- Assumes that the true model is in the candidate set
- Imposes a harsher penalty on new predictors than the AIC does

*** Information criteria and other methods

- AIC is asymptotically equivalent to leave-one-out crossvalidation
- BIC is asymptotically equivalent to k-fold crossvalidation (where the value of k depends on N)
- In the case of nested models with one parameter of difference, the AIC and BIC are equivalent to LRTs
  - \alpha = 0.16 (AIC)
  - \alpha dependent on the sample size (BIC)
  - N = 100, \alpha = 0.032

*** Summing up

- Likelihood ratio tests, ICs, and crossvalidation are all closely related
- Each has advantages and disadvantages
- All are easy to perform in R, and there is no reason not to perform and report all of them
  - Except CV
- Because they are so similar, the results are unlikely to be different in practice

** Shrinkage

- Shrinkage models: a different perspective on variable selection/model comparison
- Previous techniques have two steps
  - Fit models
  - Compare them for GoF/complexity
- In shrinkage methods, there is only one step
- The complexity penalty is part of the model fitting

*** Lasso
:PROPERTIES:
:reveal_background: lasso-ii.jpg
:END:

- LASSO penalizes non-zero regression coefficients
- Can be used to decide between highly correlated variables
- Can also be used as a general model fitting technique

#+html: <!-- Photo from https://investingcaffeine.com/tag/lasso/; original source unknown -->

*** A LASSO example

From my work on dating OE texts:

#+RESULTS: en-coef
| Name                | Value | Name                | Value | Name                    | Value |
|---------------------+-------+---------------------+-------+-------------------------+-------|
| {{{red(DiagMC)}}}   |  0.11 | VtoC                |  0.12 | {{{green(TopPPSpro)}}}  |  0.09 |
| {{{red(DiagCC)}}}   |     0 | SCan                |     0 | {{{green(TopObjSpro)}}} |     0 |
| {{{red(DiagSC)}}}   |     0 | ScrSC               |     0 | {{{green(TopPPSbj)}}}   |     0 |
| {{{red(AuxVRoot)}}} |     0 | {{{blue(NGenSbj)}}} |     0 | {{{green(TopObjSbj)}}}  |     0 |
| {{{red(AuxVSC)}}}   |     0 | {{{blue(NGenObj)}}} |  0.41 | NegCon                  |     0 |
| DiagVP              |     0 | Rel                 |  0.16 | Expl                    |     0 |

*** Another LASSO example

- In the interactive app
- Choosing between frequency measures in the lexical decision data

* Applying these methods to mixed effects models

- When we move from simple models to mixed-effects, questions arise about how to apply these methods
  - The method might apply without modification
  - The method might need tweaks to work properly
  - The method might not apply for theoretical reasons
  - The method might not apply for computational reasons

** Likelihood-based methods

- LRT, AIC, and BIC generally work for the selection of fixed effects in mixed models
- These criteria do not work for selection of random effects
  - Any models compared via these methods should have the same random effects
  - They should also be fit by ML (=REML = FALSE= in R)

** LASSO

- A version of LASSO is available for mixed-effects models
- However, itʼs very computationally intensive
  - (Why?  Setting \lambda{})
- Look at the “Mixed Model LASSO” example
  - What do you notice about the effect sizes and signs?

* Selecting fixed effects structure

#+attr_html: :height 500px
[[file:sci-method.jpg]]

** Methodological realism

- Ideal case: testing a hypothesis operationalized by one variable
  - Perfectly balanced experiment
  - Covariates controlled
- In real life, things are more complicated

** Model selection over multiple covariates

- Common scenario, especially in observational/corpus studies
  - Collect data on everything, see what analyses will stick
- Implies testing many models/hypotheses

** Problems with multiple hypothesis testing

- p-values are no longer accurate
  - Bonferroni correction, early 1960s
- Data dredging becomes a concern

** Kinds of multiple hypothesis testing

- Stepwise selection
  - Forwards
  - Backwards
- Not a good idea
- R^2, p-value, coefficient estimates all wrong

** What to do instead

- Fit a full model, explicate the effects and their sizes
- Use shrinkage model
- Model averaging
  - Bayesian or AIC-based
  - Appropriate when there are multiple competing explanations
- Do you accept what p-values mean?
  - https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/

** Data visualization

- Exploratory visualization is hugely important
- Example: unpacking the effect of lexicality last week

* Selecting random effects structure

- Most linguistic studies treat random effects as nuisance variables
  - Control for features of the data, but are not interesting
  - Contrasts with the position in other disciplines
- So why should we select on them at all?
  - Correctness
  - Fitting problems

** How to do random effects?

- Two competing approaches to the selection of random effects structures have recently appeared in the literature
  - “Keep it maximal” ([[http://talklab.psy.gla.ac.uk/KeepItMaximalR2.pdf][Barr et al. 2013]])
  - Donʼt ([[https://arxiv.org/pdf/1506.04967.pdf][Bates et al. 2015]])

** Computational note

- =lme4= has two methods for fitting models
  - REML: fit the random effects first, then fit the fixed effects
  - ML: fit both at the same time
- REML is usually better if all you care about is a single model
  - Therefore, itʼs the default
- When youʼre manipulating random effects, though, it does the wrong thing
- For these purposes, we always have to pass =REML = FALSE= to the =lmer= function

** The Barr et al. recommendation

- From linear models to LMEM: acknowledge that subjects have different baseline responses
- From random intercepts to random slopes: recognize that subjects have different responses to experimental manipulation
- We should always fit random by-subject slopes for each experimental variable of interest
  - Justfication: analogy with ANOVA, simulation

** What about by-item slopes

- In the examples Barr et al consider, the fixed effect predictors are all nested within items
  - Word class, length, etc.
- So, no need to fit by-item slopes
  - In fact, itʼs impossible
- Depending on the study predictors might be nested in participants
  - Gender, age, ...
- Does it make sense to fit these within items?

** In code

- Linear model: =outcome ~ predictor1 + predictor2=
- Random intercepts: =outcome ~ predictor1 + predictor2 + (1 | person) + (1 | word)=
- Random slopes: =outcome ~ predictor1 + predictor2 + (1 + predictor1 | person) + (1 + predictor2 | word)=
** Problems with this recommendation

- But, this makes models explode computationally
- Correlation between random effects
  - Example: quick/careless readers
- The number of correlation parameters increases proportionally to the square of the number of predictors
  - (Specifically: $N(N-1)/2$)

    \[\begin{bmatrix}
    var_{0,0} & \cdots \\
    cor_{0,1} & var_{1,1} & \cdots \\
    cor_{0,2} & cor_{1,2} & var_{2,2} & \cdots
    \end{bmatrix}
    \]

** Bates et al. reply

- Higher-order correlation terms make no discernable difference to the estimation of fixed effects
  - In several case studies from the literature
- Models that include full variance-covariance structures donʼt fit properly
  - Can fail to converge
  - Can have rank-deficient var/cov matrices even while converging
- (The Barr et al. simulations arenʼt realistic)

** Principal components analysis of the random effects

- PCA in general: a method to reduce the dimensionality of a dataset
- Here: a test whether a model is overspecified for random effects
- Any “standard deviations” of (close to) zero are bad

** Zero correlation parameter models

- Bates et al. introduce a “zero correlation parameter” model
- Sets the off-diagonal elements of the cor/cov matrix to zero
  - Doesnʼt capture the “fast and sloppy reader” case
- But, it reduces the number of parameters that need to be fitted
  - O(N^2) \to O(N)
- Convenient syntax in =lme4= for this
  - =(1 + predictor1 + predictor2 || subject)=
  - Equivalent to =(1 | subject) + (0 + predictor1 | subject) + (0 + predictor2 | subject)=

** Exercise

- Notice, in the full model, the near-1 correlation between CELEX freq and summed bigram freq
  - Indicates poor fit; parameter is close to the boundary
- Notice, in the ZCP model, that the variance estimate for =summed.bigram= is effectively zero
- Notice that these problems go away in the final model we fit
  - Though we might look with suspicion on the random intercept for =celex.frequency=
- Notice that throughout, the fixed effects estimates have not changed

** So whatʼs the big deal?

- This dataset has very favorable characteristics
- You donʼt have fitting problems
  - ...until you do
- The truth is probably somewhere in the middle
  - Intercept-only models are too sparse
  - Full models can be overspecified
- (The section on nonlinear effects is more important)

* Bayesian models

- You may hear the word “Bayesian” used in connection with modeling
- A framework for inference
- A particular simulation-based fitting procedure

** Why Bayesian

- Bayesian inference is a good fit with Monte Carlo simulation
- Monte Carlo = math is hard, letʼs go to the casino
- The popularity of Bayesian inference is at least in part due to how well Monte Carlo methods work

** Advantages of Bayesian

- Bayesian methods are incredibly flexible
- You can build your own model from building blocks
- Everything is a probability distribution
  - Hypothesis testing
  - Model comparison
  - All reduce down to statements about a distribution
- Better convergence (under some conditions)

** Disadvantages of Bayesian methods

- You gotta want it
- Building models from scratch is difficult

** A Bayesian example

#+attr_html: :style font-size:50%
#+begin_example
data {
  int<lower=0> N; // # of rats
  int<lower=0> T; // # of observations
  real x[T];      // Days elapsed
  real y[N,T];    // Weights
}
parameters {
  real alpha[N];  // Intercept
  real beta[N];   // Slope

  real mu_alpha;
  // ...
}
model {
  mu_alpha ~ normal(0, 100);
  // ...
  alpha ~ normal(mu_alpha, sigma_alpha);
  beta ~ normal(mu_beta, sigma_beta);
  for (n in 1:N)
    for (t in 1:T)
      y[n,t] ~ normal(alpha[n] + beta[n] * x[t], sigma_y);

}
#+end_example

* Resources

- Books
  - Bates 2010, “lme4: Mixed effects modeling with R” ([[http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf][link]])
  - Gelman and Hill (2006) “Data Analysis Using Regression and Multilevel/Hierarchical Models” ([[https://www.amazon.co.uk/Analysis-Regression-Multilevel-Hierarchical-Analytical/dp/052168689X/][amazon]])
  - Kruschke (2010/2014) “Doing Bayesian data analysis” ([[https://www.amazon.co.uk/Doing-Bayesian-Data-Analysis-Second/dp/B013F5JN72/][amazon]])

#+reveal: split

- Articles
  - Baayen et al 2007 “[[http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf][Mixed-effects modeling with crossed random effects for subjects and items]]”
  - Gorman 2009 “[[http://repository.upenn.edu/ircs_reports/202/][Hierarchical regression modeling for language research]]”

#+reveal: split

- Web resources
  - R-focused mailing lists: [[https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models][R-sig-ME]] and [[https://mailman.ucsd.edu/mailman/listinfo/ling-r-lang-l][R-lang]]
  - [[https://stats.stackexchange.com/][Crossvalidated]]
